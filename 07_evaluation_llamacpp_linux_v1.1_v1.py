# 07_final_evaluation_llamacpp.py

import os
import sys
import subprocess
import time
import requests
import json
import pandas as pd
import re
import random
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import datetime

# ==============================================================================
# --- 1. ç”¨æˆ·é…ç½®åŒºåŸŸ (è¯·åŠ¡å¿…æ£€æŸ¥å¹¶ä¿®æ”¹æ­¤éƒ¨åˆ†) ---
# ==============================================================================
# llama.cpp server.exe çš„å®Œæ•´è·¯å¾„
LLAMACPP_SERVER_EXE = "/root/autodl-tmp/llama.cpp/build/bin/llama-server"

# å­˜æ”¾æ‰€æœ‰ .gguf æ¨¡å‹æ–‡ä»¶çš„ç›®å½•
MODELS_DIR = "/root/autodl-tmp/paper2/models"

# è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# è¦è¯„ä¼°çš„æ¨¡å‹æ–‡ä»¶ååˆ—è¡¨ (æ‚¨å¯ä»¥æ³¨é‡Šæ‰ä¸æƒ³æµ‹è¯•çš„æ¨¡å‹)
MODELS_TO_EVALUATE = [
    "Qwen3-14B-Q4_K_M.gguf",
    "gemma-3-4b-it-Q4_K_M.gguf",
    "DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf"
]

# æ¯ä¸ªæ¨¡å‹çš„é‡å¤æµ‹è¯•æ¬¡æ•°
NUM_RUNS = 1

# llama.cpp æœåŠ¡å™¨å‚æ•°
SERVER_HOST = "127.0.0.1"
SERVER_PORT = 8080
SERVER_THREADS = 36
SERVER_CTX_SIZE = 8192 # 8k ä¸Šä¸‹æ–‡
SERVER_GPU_LAYERS = 99 # å°½å¯èƒ½å¤šåœ°ä½¿ç”¨GPU

LLAMACPP_API_URL = f"http://{SERVER_HOST}:{SERVER_PORT}/v1/chat/completions"
LLAMACPP_HEALTH_URL = f"http://{SERVER_HOST}:{SERVER_PORT}/health"
MODELS_URL          = f"http://{SERVER_HOST}:{SERVER_PORT}/v1/models"
os.makedirs('logs', exist_ok=True)
ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
log_path = os.path.join('logs', f'smoke_test_{ts}.log')

# ------------------------- Tee æ—¥å¿—ç±» -------------------------
class Tee:
    def __init__(self, filename):
        self.file = open(filename, 'w', encoding='utf-8')
        self.stdout = sys.stdout
        sys.stdout = self
    def write(self, data):
        self.file.write(data)
        self.stdout.write(data)
    def flush(self):
        self.file.flush()
        self.stdout.flush()

Tee(log_path)

# ==============================================================================
# --- 2. æ—¥å¿—ä¸ä»»åŠ¡é…ç½® ---
# ==============================================================================
os.makedirs('logs', exist_ok=True)
os.makedirs('results', exist_ok=True)
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
log_filename = f'log_final_llamacpp_eval_{timestamp}.txt'

class Logger(object):
    def __init__(self, filename): self.terminal = sys.stdout; self.log = open(filename, 'a', encoding='utf-8')
    def write(self, message): self.terminal.write(message); self.log.write(message)
    def flush(self): self.terminal.flush(); self.log.flush()

sys.stdout = Logger(os.path.join('logs', log_filename))
sys.stderr = sys.stdout
print(f"--- ç»ˆæè¯„ä¼°æµæ°´çº¿ (llama.cpp) ---\nTimestamp: {timestamp}\n")

TASKS_TO_EVALUATE = {
    'fibrosis': {'column_name': 'Fibrosis_Stage_0_4', 'name': 'è‚çº¤ç»´åŒ–åˆ†æœŸ', 'labels': [0, 1, 2, 3, 4]},
    'inflammation': {'column_name': 'Inflammation_Grade_0_4', 'name': 'è‚è„ç‚ç—‡åˆ†çº§', 'labels': [0, 1, 2, 3, 4]},
    'steatosis': {'column_name': 'Steatosis_Grade_1_3', 'name': 'è‚è„‚è‚ªå˜æ€§åˆ†çº§', 'labels': [1, 2, 3]}
}

# ==============================================================================
# --- 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---
# ==============================================================================
def get_system_prompt(task_name, labels_str): return f"ä½ æ˜¯ä¸€ä½é¡¶å°–çš„è‚è„ç—…ç†å­¦ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç—…ç†æè¿°å¯¹â€œ{task_name}â€è¿›è¡Œåˆ†çº§ã€‚ä½ çš„å›ç­”å¿…é¡»ä¸¥æ ¼éµå®ˆè§„åˆ™ï¼šåªè¾“å‡ºä¸€ä¸ªå±äº {labels_str} çš„é˜¿æ‹‰ä¼¯æ•°å­—ï¼Œç¦æ­¢ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"
def get_user_prompt(text, examples, task_name, labels_str):
    example_str = "".join([f"[ç¤ºä¾‹{i+1}]\nç—…ç†æè¿°ï¼š\n\"{ex['text']}\"\n{task_name} ({labels_str}):\n{ex['label']}\n\n" for i, ex in enumerate(examples)])
    return f"è¯·å‚è€ƒä»¥ä¸‹ç¤ºä¾‹ï¼Œå¹¶å¯¹æ–°çš„ç—…ç†æè¿°è¿›è¡Œåˆ¤æ–­ã€‚\n\n{example_str}æ–°çš„ç—…ç†æè¿°ï¼š\n\"{text}\""

# def parse_response(response_text, labels):
#     if response_text == "LLM_CALL_ERROR":
#         return -1
#     # å»æ‰æ‰€æœ‰ <think>...</think> æ ‡ç­¾
#     cleaned = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()
#     numbers = re.findall(r'\b([0-4])\b', cleaned)   # åªæŠ“ 0-4 çš„ç‹¬ç«‹æ•°å­—
#     if numbers:
#         grade = int(numbers[-1])
#         return grade if grade in labels else -3
#     return -2

def parse_response(text: str, labels):
    """
    ç»Ÿä¸€è§£æ 0-4 åˆ†æœŸçš„æœ€ç»ˆæ•°å­—
    """
    # 1ï¸âƒ£ ä¼˜å…ˆåŒ¹é… <answer>...</answer>
    m = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL | re.IGNORECASE)
    if m:
        cleaned = m.group(1).strip()
    else:
        # 2ï¸âƒ£ å»æ‰ <think>...</think> åå‰©ä½™å†…å®¹
        cleaned = re.sub(r'<think>.*?</think>', '', text,
                         flags=re.DOTALL | re.IGNORECASE).strip()
    
    # 3ï¸âƒ£ æŠ½å–ç‹¬ç«‹æ•°å­— 0-4
    nums = re.findall(r'\b([0-4])\b', cleaned)
    return int(nums[-1]) if nums and int(nums[-1]) in labels else None

def get_llm_response_llamacpp(system_prompt, user_prompt, model_filename,
                              max_retries: int = 3,
                              base_timeout: int = 60):
    """
    å¸¦æŒ‡æ•°é€€é¿ + è¶…æ—¶é€’å¢çš„ llama.cpp API è°ƒç”¨
    """
    headers = {"Content-Type": "application/json"}
    payload = {
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user",   "content": user_prompt}
        ],
        "temperature": 0.0
    }

    for attempt in range(1, max_retries + 1):
        # è¶…æ—¶æ—¶é—´çº¿æ€§é€’å¢ï¼š60 â†’ 120 â†’ 180
        timeout_now = base_timeout + (attempt - 1) * 60
        try:
            response = requests.post(
                LLAMACPP_API_URL,
                headers=headers,
                data=json.dumps(payload),
                timeout=timeout_now
            )
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content'].strip()

        except requests.exceptions.ReadTimeout as e:
            print(f"[Retry {attempt}/{max_retries}] ReadTimeout ({e}) â†’ "
                  f"timeout={timeout_now}s, back-off {2 ** attempt}s")
            if attempt == max_retries:
                print("æœ€ç»ˆå¤±è´¥ï¼Œæ ‡è®°ä¸º LLM_CALL_ERROR")
                return "LLM_CALL_ERROR"
            time.sleep(2 ** attempt + random.uniform(0, 1))   # æŒ‡æ•°é€€é¿ + æŠ–åŠ¨

        except requests.exceptions.RequestException as e:
            # å…¶å®ƒç½‘ç»œé”™è¯¯ä¹Ÿé‡è¯•
            print(f"[Retry {attempt}/{max_retries}] {e}")
            if attempt == max_retries:
                return "LLM_CALL_ERROR"
            time.sleep(2 ** attempt + random.uniform(0, 1))

# def wait_for_server_ready(timeout=60):
#     start_time = time.time()
#     while time.time() - start_time < timeout:
#         try:
#             response = requests.get(LLAMACPP_HEALTH_URL, timeout=1)
#             if response.status_code == 200 and response.json().get("status") == "ok":
#                 print("\næœåŠ¡å™¨å·²å°±ç»ªï¼")
#                 return True
#         except requests.exceptions.RequestException: pass
#         time.sleep(1)
#     print("\né”™è¯¯ï¼šç­‰å¾…æœåŠ¡å™¨è¶…æ—¶ï¼")
#     return False

def log(msg):
    """ç»Ÿä¸€æ—¥å¿—å‡½æ•°"""
    ts = datetime.datetime.now().strftime("%H:%M:%S")
    print(f"[{ts}] {msg}")

def wait_for_server_ready(timeout=300):
    """ç­‰å¾…æœåŠ¡å™¨å°±ç»ªï¼ˆä¿®å¤ç‰ˆï¼‰"""
    start_time = time.time()
    attempts = 0
    while time.time() - start_time < timeout:
        attempts += 1
        try:
            # 1. æ£€æŸ¥ /health çŠ¶æ€
            health_r = requests.get(LLAMACPP_HEALTH_URL, timeout=5)
            if health_r.status_code == 200:
                health_data = health_r.json()
                if health_data.get("status") != "ok":
                    log(f"ğŸŸ¡ å°è¯•#{attempts} /health æœªå°±ç»ª: {health_data}")
                    time.sleep(3)
                    continue
            else:
                log(f"ğŸŸ¡ å°è¯•#{attempts} /health çŠ¶æ€ç : {health_r.status_code}")
                time.sleep(3)
                continue
                
            # 2. æ£€æŸ¥ /v1/models çŠ¶æ€
            models_r = requests.get(MODELS_URL, timeout=5)
            if models_r.status_code == 200:
                models_data = models_r.json()
                if models_data.get("object") == "list" and len(models_data.get("data", [])) > 0:
                    model_id = models_data["data"][0]["id"]
                    log(f"ğŸŸ¢ æ¨¡å‹åŠ è½½å®Œæˆ: {model_id}")
                    return True
            log(f"ğŸŸ¡ å°è¯•#{attempts} /models æœªå°±ç»ª: {models_r.status_code} {models_r.text[:100]}")
            
        except requests.exceptions.ConnectionError:
            log(f"ğŸŸ¡ å°è¯•#{attempts} è¿æ¥æ‹’ç»ï¼Œç­‰å¾…å¯åŠ¨...")
        except Exception as e:
            log(f"âš ï¸ å°è¯•#{attempts} æ£€æŸ¥é”™è¯¯: {str(e)}")
        
        time.sleep(5)
    log(f"âŒ ç­‰å¾…è¶…æ—¶({timeout}ç§’)")
    return False

# ==============================================================================
# --- 4. ä¸»æ‰§è¡Œé€»è¾‘ ---
# ==============================================================================
if __name__ == "__main__":
    if not os.path.exists(LLAMACPP_SERVER_EXE):
        print(f"è‡´å‘½é”™è¯¯ï¼šæ‰¾ä¸åˆ° llama-serverï¼Œè·¯å¾„ä¸æ­£ç¡®: {LLAMACPP_SERVER_EXE}"); exit()
    if not os.path.exists(MODELS_DIR):
        print(f"è‡´å‘½é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹ç›®å½•ï¼Œè·¯å¾„ä¸æ­£ç¡®: {MODELS_DIR}"); exit()
        
    val_df   = pd.read_csv(os.path.join(SCRIPT_DIR, 'processed_data', 'val_set.csv'))
    train_df = pd.read_csv(os.path.join(SCRIPT_DIR, 'processed_data', 'train_set.csv'))
    print(f"æˆåŠŸåŠ è½½æ•°æ®é›†ã€‚\n")
    
    all_runs_data = []

    for run_num in range(1, NUM_RUNS + 1):
        for model_filename in MODELS_TO_EVALUATE:
            model_path = os.path.join(MODELS_DIR, model_filename)
            if not os.path.exists(model_path):
                print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}ï¼Œè·³è¿‡æ­¤æ¨¡å‹ã€‚")
                continue

            print(f"\n\n{'='*70}\n===== å¼€å§‹ç¬¬ {run_num}/{NUM_RUNS} è½®æµ‹è¯• | æ¨¡å‹: {model_filename} =====\n{'='*70}")
            
            # --- è‡ªåŠ¨å¯åŠ¨æœåŠ¡å™¨ ---
            server_command = [
                LLAMACPP_SERVER_EXE,
                "--model", model_path,
                "--threads", str(SERVER_THREADS),
                "--ctx-size", str(SERVER_CTX_SIZE),
                "--host", SERVER_HOST,
                "--port", str(SERVER_PORT),
                "--n-gpu-layers", str(SERVER_GPU_LAYERS),
                "--flash-attn"
            ]
            server_proc = None
            try:
                print("æ­£åœ¨å¯åŠ¨ llama.cpp æœåŠ¡å™¨...")
                server_proc = subprocess.Popen(server_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                
                # if not wait_for_server_ready(): continue # å¦‚æœæœåŠ¡å™¨å¯åŠ¨å¤±è´¥ï¼Œè·³åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹
                if not wait_for_server_ready():
                    log("  âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥ï¼Œè·³è¿‡")
                    subprocess.run(["pkill", "-f", "llama-server"]); 
                    continue
            # æ˜¾ç¤ºcmdæŠ¥é”™å†…å®¹
            # try:
            #     print("æ­£åœ¨å¯åŠ¨ llama.cpp æœåŠ¡å™¨...")
            #     # æ ¸å¿ƒä¿®æ”¹ï¼šç§»é™¤stdoutå’Œstderrçš„PIPEï¼Œè®©æœåŠ¡å™¨ä¿¡æ¯ç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°
            #     server_proc = subprocess.Popen(server_command) 
                
            #     if not wait_for_server_ready(): 
            #         # å¦‚æœç­‰å¾…å¤±è´¥ï¼Œæˆ‘ä»¬æ‰‹åŠ¨ç»ˆæ­¢å¯èƒ½å¡ä½çš„è¿›ç¨‹
            #         if server_proc:
            #             server_proc.terminate()
            #         continue

                # --- æœåŠ¡å™¨å°±ç»ªï¼Œå¼€å§‹å¯¹æ‰€æœ‰ä»»åŠ¡è¿›è¡Œè¯„ä¼° ---
                for task_key, task_config in TASKS_TO_EVALUATE.items():
                    task_start_time = time.time()
                    task_name, col_name, labels = task_config['name'], task_config['column_name'], task_config['labels']
                    print(f"\n\n--- å¼€å§‹ä»»åŠ¡: {task_name} ---")
                    
                    labels_str = ", ".join(map(str, labels))
                    few_shot_examples = [{'text': r['ç—…ç†æè¿°_åˆ é™¤è¯Šæ–­'], 'label': r[col_name]} for _, r in train_df.groupby(col_name).head(1).iterrows() if r[col_name] in labels]
                    system_prompt = get_system_prompt(task_name, labels_str)
                    
                    predictions, true_labels, sample_times = [], [], []
                    for i, row in enumerate(val_df.itertuples()):
                        text, true_label = row.ç—…ç†æè¿°_åˆ é™¤è¯Šæ–­, getattr(row, col_name)
                        if pd.isna(true_label): continue
                        
                        sample_start_time = time.time()
                        user_prompt = get_user_prompt(text, few_shot_examples, task_name, labels_str)
                        response_text = get_llm_response_llamacpp(system_prompt, user_prompt, model_filename)
                        predicted_label = parse_response(response_text, labels)
                        sample_end_time = time.time()
                        
                        sample_duration = sample_end_time - sample_start_time
                        predictions.append(predicted_label); true_labels.append(int(true_label)); sample_times.append(sample_duration)
                        print(f"  > è¿›åº¦ {i+1}/{len(val_df)} | çœŸå®: {int(true_label)} | é¢„æµ‹: {predicted_label} | è€—æ—¶: {sample_duration:.2f}s")

                    # --- åˆ†æå•ä¸ªä»»åŠ¡ç»“æœ ---
                    valid_indices = [i for i, p in enumerate(predictions) if p is not None]
                    if len(valid_indices) > 0:
                        accuracy = accuracy_score([true_labels[i] for i in valid_indices], [predictions[i] for i in valid_indices])
                        macro_f1 = f1_score([true_labels[i] for i in valid_indices], [predictions[i] for i in valid_indices], average='macro', zero_division=0)
                    else:
                        accuracy, macro_f1 = 0, 0
                    
                    all_runs_data.append({
                        "run": run_num, "model": model_filename, "task_column": col_name,
                        "accuracy": accuracy, "macro_f1": macro_f1,
                        "error_rate": (len(predictions) - len(valid_indices)) / len(predictions),
                        "avg_time_per_sample": sum(sample_times) / len(sample_times) if sample_times else 0
                    })
                    # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
                    # åˆ›å»ºä¸€ä¸ªä¸“é—¨å­˜æ”¾è¯¦ç»†ç»“æœçš„æ–‡ä»¶å¤¹
                    predictions_dir = os.path.join('results', 'detailed_predictions')
                    os.makedirs(predictions_dir, exist_ok=True)

                    # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
                    detailed_results = []
                    for i in range(len(val_df)):
                        # ä»åŸå§‹DataFrameè·å–Patient_IDå’Œç—…ç†æè¿°
                        patient_id = val_df.iloc[i]['Patient_ID']
                        pathology_text = val_df.iloc[i]['ç—…ç†æè¿°_åˆ é™¤è¯Šæ–­']
                        
                        # è·å–è¯¥æ ·æœ¬çš„çœŸå®æ ‡ç­¾å’Œé¢„æµ‹ç»“æœ
                        true_label_for_sample = true_labels[i] if i < len(true_labels) else None
                        predicted_label_for_sample = predictions[i] if i < len(predictions) else None
                        
                        detailed_results.append({
                            'Patient_ID': patient_id,
                            'True_Label': true_label_for_sample,
                            'Predicted_Label': predicted_label_for_sample,
                            'Task': col_name,
                            'Pathology_Text': pathology_text
                        })

                    # å°†è¯¦ç»†ç»“æœä¿å­˜ä¸ºCSV
                    model_name_for_file = os.path.splitext(model_filename)[0]
                    output_filename = f"preds_{model_name_for_file}_task_{task_key}_run_{run_num}.csv"
                    output_path = os.path.join(predictions_dir, output_filename)

                    pd.DataFrame(detailed_results).to_csv(output_path, index=False, encoding='utf-8-sig')
                    print(f"  -> è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {output_path}")
                    
            finally:
                # --- è‡ªåŠ¨å…³é—­æœåŠ¡å™¨ ---
                if server_proc:
                    print("\næ­£åœ¨å…³é—­ llama.cpp æœåŠ¡å™¨...")
                    server_proc.terminate()
                    try:
                        server_proc.wait(timeout=30)        # ç»™ 30 ç§’
                    except subprocess.TimeoutExpired:
                        print("æœåŠ¡å™¨æœªåœ¨ 30 ç§’å†…é€€å‡ºï¼Œå¼ºåˆ¶ kill")
                        subprocess.run(["pkill", "-f", "llama-server"]); 
                    print("æœåŠ¡å™¨å·²å…³é—­ã€‚")

    # --- 5. ä¿å­˜å¹¶åˆ†ææœ€ç»ˆç»“æœ ---
    print("\n\n" + "="*70)
    print("===== æ‰€æœ‰è¯„ä¼°è½®æ¬¡å®Œæˆï¼å¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š... =====")
    print("="*70)

    if all_runs_data:
        # ä¿å­˜åŒ…å«æ‰€æœ‰è½®æ¬¡ç»†èŠ‚çš„åŸå§‹æ•°æ®
        raw_df = pd.DataFrame(all_runs_data)
        raw_csv_path = os.path.join('results', f'raw_results_{timestamp}.csv')
        raw_df.to_csv(raw_csv_path, index=False, encoding='utf-8-sig')
        print(f"\nåŸå§‹æ•°æ®å·²ä¿å­˜è‡³: {raw_csv_path}")

        # è®¡ç®—èšåˆç»Ÿè®¡æ•°æ® (å‡å€¼å’Œæ ‡å‡†å·®)
        aggregated_df = raw_df.groupby(['model', 'task_column']).agg(
            f1_mean=('macro_f1', 'mean'),
            f1_std=('macro_f1', 'std'),
            acc_mean=('accuracy', 'mean'),
            acc_std=('accuracy', 'std'),
            time_mean=('avg_time_per_sample', 'mean')
        ).reset_index()

        # ä¿å­˜èšåˆåçš„æ‘˜è¦æ•°æ®
        agg_csv_path = os.path.join('results', f'aggregated_summary_{timestamp}.csv')
        aggregated_df.to_csv(agg_csv_path, index=False, encoding='utf-8-sig')
        print(f"\nèšåˆç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜è‡³: {agg_csv_path}\n")

        print("--- æœ€ç»ˆèšåˆç»“æœ ---")
        print(aggregated_df.to_string())
    else:
        print("æ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœã€‚")